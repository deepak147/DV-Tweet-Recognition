{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lybzWLVfcMv4"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9uoSja80cMwV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4149, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dv data\n",
    "data = pd.read_csv('dvdata.csv', header = None, delimiter=',')\n",
    "data.columns = ['Id', 'Text']\n",
    "\n",
    "#nondv data\n",
    "dt=pd.read_csv('notdv.csv', header = None, delimiter=',')\n",
    "dt.columns = ['Id', 'Text']\n",
    "\n",
    "Label=[]\n",
    "for l in data.Id:\n",
    "    Label.append(1)\n",
    "data['Label']=Label\n",
    "\n",
    "Label=[]\n",
    "for l in dt.Id:\n",
    "    Label.append(0)\n",
    "dt['Label']=Label\n",
    "        \n",
    "dvdt = pd.concat([data, dt], axis=0, ignore_index = True)\n",
    "pos = []\n",
    "neg = []\n",
    "for l in dvdt.Label:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "dvdt['Pos']= pos\n",
    "dvdt['Neg']= neg\n",
    "dvdt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wyEuqfH9cMwe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deepak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate everything</td>\n",
       "      <td>[hate, everything]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foul deeds rise fireopal19 darkest dark crime ...</td>\n",
       "      <td>[foul, deeds, rise, fireopal19, darkest, dark,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecobard ‚Äô let pass everything make substantiat...</td>\n",
       "      <td>[ecobard, ‚Äô, let, pass, everything, make, subs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rulebc15 theprojecttv stop treating dv gendere...</td>\n",
       "      <td>[rulebc15, theprojecttv, stop, treating, dv, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>observer view domestic violence observer edito...</td>\n",
       "      <td>[observer, view, domestic, violence, observer,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>2Ô∏è‚É£0Ô∏è‚É£ vintage ü§ù ive teamed awesome folks subs...</td>\n",
       "      <td>[2Ô∏è‚É£0Ô∏è‚É£, vintage, ü§ù, ive, teamed, awesome, fol...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>adoptionsuk looks gorgeous digital artist happ...</td>\n",
       "      <td>[adoptionsuk, looks, gorgeous, digital, artist...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4146</th>\n",
       "      <td>comes unicorn squad artwork turnipberry find h...</td>\n",
       "      <td>[comes, unicorn, squad, artwork, turnipberry, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4147</th>\n",
       "      <td>youfine dropping soon mampm maxistarpasachy ar...</td>\n",
       "      <td>[youfine, dropping, soon, mampm, maxistarpasac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>coronavirusoutbreak boring really life football</td>\n",
       "      <td>[coronavirusoutbreak, boring, really, life, fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4149 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Text_Final  \\\n",
       "0                                       hate everything   \n",
       "1     foul deeds rise fireopal19 darkest dark crime ...   \n",
       "2     ecobard ‚Äô let pass everything make substantiat...   \n",
       "3     rulebc15 theprojecttv stop treating dv gendere...   \n",
       "4     observer view domestic violence observer edito...   \n",
       "...                                                 ...   \n",
       "4144  2Ô∏è‚É£0Ô∏è‚É£ vintage ü§ù ive teamed awesome folks subs...   \n",
       "4145  adoptionsuk looks gorgeous digital artist happ...   \n",
       "4146  comes unicorn squad artwork turnipberry find h...   \n",
       "4147  youfine dropping soon mampm maxistarpasachy ar...   \n",
       "4148    coronavirusoutbreak boring really life football   \n",
       "\n",
       "                                                 tokens  Label  Pos  Neg  \n",
       "0                                    [hate, everything]      1    1    0  \n",
       "1     [foul, deeds, rise, fireopal19, darkest, dark,...      1    1    0  \n",
       "2     [ecobard, ‚Äô, let, pass, everything, make, subs...      1    1    0  \n",
       "3     [rulebc15, theprojecttv, stop, treating, dv, g...      1    1    0  \n",
       "4     [observer, view, domestic, violence, observer,...      1    1    0  \n",
       "...                                                 ...    ...  ...  ...  \n",
       "4144  [2Ô∏è‚É£0Ô∏è‚É£, vintage, ü§ù, ive, teamed, awesome, fol...      0    0    1  \n",
       "4145  [adoptionsuk, looks, gorgeous, digital, artist...      0    0    1  \n",
       "4146  [comes, unicorn, squad, artwork, turnipberry, ...      0    0    1  \n",
       "4147  [youfine, dropping, soon, mampm, maxistarpasac...      0    0    1  \n",
       "4148  [coronavirusoutbreak, boring, really, life, fo...      0    0    1  \n",
       "\n",
       "[4149 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]\n",
    "\n",
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "\n",
    "dvdt['Text_Clean'] = dvdt['Text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#lemmatization\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "tokens = [word_tokenize(sen) for sen in dvdt.Text_Clean]\n",
    "lower_tokens = [lower_token(token) for token in tokens]\n",
    "\n",
    "#stopword_removal\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "filtered_words = [remove_stop_words(sen) for sen in lower_tokens]\n",
    "result = [' '.join(sen) for sen in filtered_words]\n",
    "dvdt['Text_Final'] = result\n",
    "dvdt['tokens'] = filtered_words\n",
    "\n",
    "dvdt = dvdt[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]\n",
    "dvdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jBPo-mt7cMwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49895 words total, with a vocabulary size of 15643\n",
      "Max sentence length is 60\n",
      "12833 words total, with a vocabulary size of 5954\n",
      "Max sentence length is 44\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = train_test_split(dvdt, test_size=0.20, random_state=42)\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Gv6mwHHccMwk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15643 unique tokens.\n",
      "(15644, 300)\n"
     ]
    }
   ],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=100000)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GFSh6iW_cMwm"
   },
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MictBIfYcMwo",
    "outputId": "04970a50-8196-4bc5-a7df-66cc0483869e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3319, 50) (3319, 2)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 50, 300)      4693200     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 49, 200)      120200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 48, 200)      180200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 47, 200)      240200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 46, 200)      300200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 45, 200)      360200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 200)         0           ['conv1d[0][0]']                 \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 200)         0           ['conv1d_1[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 200)         0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 200)         0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 200)         0           ['conv1d_4[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1000)         0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]', \n",
      "                                                                  'global_max_pooling1d_3[0][0]', \n",
      "                                                                  'global_max_pooling1d_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1000)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          128128      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            258         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,022,586\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 4,693,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_names = ['Pos', 'Neg']\n",
    "y_train = data_train[label_names].values\n",
    "\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "print(x_train.shape, y_tr.shape)\n",
    "\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2lx5M-zscMwp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "79/79 [==============================] - 8s 88ms/step - loss: 0.3764 - acc: 0.8328 - val_loss: 0.2239 - val_acc: 0.8991\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 0.1437 - acc: 0.9390 - val_loss: 0.1616 - val_acc: 0.9352\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 0.0800 - acc: 0.9699 - val_loss: 0.1803 - val_acc: 0.9367\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 34\n",
    "\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.2, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pbMMIf_dcMwq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 584ms/step\n",
      "0.9313253012048193\n",
      "[[420  22]\n",
      " [ 35 353]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "\n",
    "labels = [1, 0]\n",
    "\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "    \n",
    "print(accuracy_score(data_test.Label.values, prediction_labels))\n",
    "print(confusion_matrix(data_test.Label.values, prediction_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a tweet : husband hurts\n",
      "Prediction :  1\n"
     ]
    }
   ],
   "source": [
    "inputQuestion = remove_punct(input('Enter a tweet : '))\n",
    "test_sequence = tokenizer.texts_to_sequences([inputQuestion])\n",
    "test_sen = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "prediction = model.predict(test_sen)\n",
    "labels = [1, 0]\n",
    "pred = labels[np.argmax(prediction)]\n",
    "print('Prediction : ', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "with open('tokenizer.pickle', 'wb') as token:\n",
    "    pickle.dump(tokenizer, token)\n",
    "with open('max_seq_len.txt', 'w') as maxseq:\n",
    "    maxseq.write(str(MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter tweet : happy birthday\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict(tweet):\n",
    "    \n",
    "    # load tokenizer\n",
    "    with open('tokenizer.pickle', 'rb') as token:\n",
    "        tokenizer = pickle.load(token)\n",
    "    with open('max_seq_len.txt', 'r') as maxseq:\n",
    "        MAX_SEQUENCE_LENGTH = maxseq.read()\n",
    "        print()\n",
    "        MAX_SEQUENCE_LENGTH = int(MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "        \n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    model.load_weights(\"model.h5\")\n",
    "    \n",
    "    test_sequence = tokenizer.texts_to_sequences([tweet])\n",
    "    test_sen = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    prediction = model.predict(test_sen)\n",
    "    labels = [1, 0]\n",
    "    pred = labels[np.argmax(prediction)]\n",
    "    \n",
    "    # prediction\n",
    "    if pred == 1:\n",
    "        print('Yes')\n",
    "    else:\n",
    "        print('No')\n",
    "        \n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweet = input('Enter tweet : ')\n",
    "    predict(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "domesticViolence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
